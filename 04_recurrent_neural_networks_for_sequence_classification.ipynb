{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks for sequence classification with TensorFlow Eager\n",
    "----\n",
    "\n",
    "In this tutorial, we are going to build a recurrent neural network (RNN) for sequence classification. \n",
    "\n",
    "The goal of the RNN will be to classify the sentiment of a movie review. We will be training this model using the IMDB sentiment analysis dataset ([dataset source](http://ai.stanford.edu/~amaas/data/sentiment/)).\n",
    "\n",
    "To download the dataset, you just have to run the **get_datasets.sh** script in the **datasets** folder, from your terminal. This script will automatically download and extract the zipped file.\n",
    "\n",
    "> **chmod o+x datasets/get_datasets.sh** \n",
    "\n",
    "> **datasets/get_datasets.sh**\n",
    "\n",
    "This dataset comes with 25000 movie reviews for training and 25000 reviews for testing. Let's jump in the data analysis :)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import here useful libraries\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mada/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mada/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Data processing and pathnames handling\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Libraries for text processing\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "\n",
    "# Import function to write data to tfrecords\n",
    "from data_utils import text2tfrecords\n",
    "\n",
    "# Import TensorFlow and TensorFlow Eager\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable eager mode. Once activated it cannot be reversed! Run just once.\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis on the IMDB dataset\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filenames of the positive/negative reviews we will use for training the RNN\n",
    "train_pos_files = glob.glob('datasets/aclImdb/train/pos/*')\n",
    "train_neg_files = glob.glob('datasets/aclImdb/train/neg/*')\n",
    "\n",
    "# Concatenate both positive and negative reviews filenames\n",
    "train_files = train_pos_files + train_neg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number positive reviews used for training:  12500\n",
      "Number negative reviews used for training:  12500\n"
     ]
    }
   ],
   "source": [
    "print('Number positive reviews used for training: ', len(train_pos_files))\n",
    "print('Number negative reviews used for training: ', len(train_neg_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a positive review:\n",
      "\n",
      " Rented the movie as a joke. My friends and I had so much fun laughing at it that I went and found a used copy and bought it for myself. Now when all my friends are looking for a funny movie I give them Sasquatch Hunters. It needs to be said though there is a rule that was made that made the movie that much better. No talking is allowed while the movie is on unless the words are Sasquatch repeated in a chant. I loved the credit at the end of the movie as well. \"Thanks for the Jeep, Tom!\" Whoever Tom is I say thank you because without your Jeep the movie may not have been made. In short a great movie if you are looking for something to laugh at. If you want a good movie maybe look for something else but if you don't mind a laugh at the expense of a man in a monkey suit grab yourself a copy.\n"
     ]
    }
   ],
   "source": [
    "# Read a positive review\n",
    "print('Example of a positive review:\\n\\n', open(train_pos_files[5],'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a negative review:\n",
      "\n",
      " The Good: I liked this movie because it was the first horror movie I've seen in a long time that actually scared me. The acting wasn't too bad, and the \"Cupid\" killer was believable and disturbing.<br /><br />The Bad: The story line and plot of this movie is incredibly weak. There just wasn't much to it. The ways the killer killed his victims was very horrifying and disgusting. I do not recommend this movie to anyone who can not handle gore.<br /><br />Overall: A good scare, but a bad story.<br /><br />** out of *****\n"
     ]
    }
   ],
   "source": [
    "# Read a negative review\n",
    "print('Example of a negative review:\\n\\n', open(train_neg_files[7070],'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a word vocabulary\n",
    "----\n",
    "As you can see in the example above, the text comes along with HTML tags as well. We will have to remove these during text processing. For each review in part, we will perform the following data cleaning tasks:\n",
    "\n",
    "* *Strip any HTML tag in the review*\n",
    "* *Use the word tokenizer to extract the words from the review. Example:*\n",
    "    > **word_tokenize(\"I can't believe I wasted my time with this movie.\")** &rarr; *['I', 'ca', \"n't\", 'believe', 'I', 'wasted', 'my', 'time', 'with', 'this', 'movie', '.']*\n",
    "* Create list of words \n",
    "* Replace words that appear less than the minimum set frequency, with **< Unknown >** token\n",
    "* Add a **< Start >** and **< End >** token to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with all the reviews in the train dataset\n",
    "reviews = [open(train_files[i],'r').read() for i in range(len(train_files))]\n",
    "\n",
    "# Remove HTML tags\n",
    "reviews = [re.sub(r'<[^>]+>', ' ', review) for review in reviews]\n",
    "\n",
    "# Tokenize each review in part\n",
    "reviews = [word_tokenize(review) for review in reviews]\n",
    "\n",
    "# Flatten nested list\n",
    "reviews = [word for review in reviews for word in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocabulary:  34819\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency of each word\n",
    "word_frequency = pd.value_counts(reviews)\n",
    "\n",
    "# Keep only words with frequency higher than minimum\n",
    "min_frequency = 5\n",
    "vocabulary = word_frequency[word_frequency>=min_frequency].index.tolist()\n",
    "\n",
    "# Add Unknown, Start and End token. \n",
    "extra_tokens = ['Unknown_token', 'Start_token', 'End_token']\n",
    "vocabulary += extra_tokens\n",
    "\n",
    "print('Number of words in the vocabulary: ', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word2idx dictionary\n",
    "word2idx = {vocabulary[i]: i for i in range(len(vocabulary))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to TFRecords\n",
    "----\n",
    "I am lucky enough to have a computer with 32GB RAM so holding the train and test dataset in memory is definitely not a problem for me. However, I do realize that some of you might struggle with RAM capabilities so I am going to make it easier for you :).\n",
    "\n",
    "We are going to create two tfrecords datasets: one for training and one for testing. Then, we are going to read the data in batches from disk using the Dataset iterator.\n",
    "\n",
    "This will provide us with two main advantages:\n",
    "* no constraints on your RAM capabilities\n",
    "* the ability to pad the variable length sequences, on the fly, within a batch\n",
    "\n",
    "Honestly, most of the real-world datasets are too big to fit into memory so I believe it is good practice to learn how to deal with such scenarios :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write train data to tfrecords.This might take a while (~10 minutes)\n",
    "train_writer = tf.python_io.TFRecordWriter('datasets/aclImdb/train.tfrecords')\n",
    "text2tfrecords(train_files, train_writer, word2idx, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filenames of the reviews we will use for testing the RNN \n",
    "test_pos_files = glob.glob('datasets/aclImdb/test/pos/*')\n",
    "test_neg_files = glob.glob('datasets/aclImdb/test/neg/*')\n",
    "test_files = test_pos_files + test_neg_files\n",
    "\n",
    "# Write test data to tfrecords (~10 minutes)\n",
    "test_writer = tf.python_io.TFRecordWriter('datasets/aclImdb/test.tfrecords')\n",
    "text2tfrecords(test_files, test_writer, word2idx, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFDataset Iterator\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset('datasets/aclImdb/train.tfrecords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
